# Прежде чем перейти к векторному представлению слов, проводится знакомая вам предобработка текста:
# Выполняют токенизацию каждого текста, то есть его разбивают на слова;
# Слова лемматизируют: приводят к начальной словарной форме (более сложные модели, например,
# BERT, этого не требуют: они сами понимают формы слов);

# Текст очищают от стоп-слов и ненужных символов;

# Для корректной работы алгоритма добавляют маркеры начала и конца предложения (они приравниваются к токенам).

# На выходе у каждого исходного текста образуется свой список токенов.
# Затем токены передают модели, которая переводит их в векторные представления. Для этого модель
# обращается к составленному заранее словарю токенов. На выходе для каждого текста образуются векторы заданной длины.
# На финальном этапе модели передают признаки (векторы). И она прогнозирует эмоциональную
# окраску текста — 0 («отрицательная») или 1 («положительная»).