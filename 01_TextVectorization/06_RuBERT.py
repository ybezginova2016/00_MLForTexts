# Прежде чем перевести тексты в векторы, подготовим их.
# У RuBERT есть собственный токенизатор. Это инструмент, который
# разбивает и преобразует исходные тексты в список токенов,
# которые есть, например, в словаре RuBERT. Лемматизация не требуется.
# Начинаем предобработку текстов:

# 1) Инициализируем токенизатор как объект класса BertTokenizer().
# Передадим ему аргумент vocab_file — это файл со словарём, на
# котором обучалась модель. Он может быть, например, в текстовом
# формате (txt).

import torch
import transformers
import numpy as np

tokenizer = transformers.BertTokenizer(
    vocab_file='/datasets/ds_bert/vocab.txt')

# 2) Преобразуем текст в номера токенов из словаря методом encode() (англ. «закодировать»):
tokenizer.encode('Очень удобно использовать уже готовый трансформатор текста', add_special_tokens=True)
# [101, 1094, 4980, 3373, 1034, 6037, 323, 73634, 10316, 102]

# Для корректной работы модели мы указали аргумент add_special_tokens
# (англ. «добавить специальные токены»), равный True. Это значит,
# что к любому преобразуемому тексту добавляется токен начала (101)
# и токен конца текста (102).
# Применим метод padding (англ. «отступ»), чтобы после токенизации
# длины исходных текстов в корпусе были равными. Только при таком
# условии будет работать модель BERT. Пусть стандартной длиной
# вектора n будет длина наибольшего во всём датасете вектора.
# Остальные векторы дополним нулями:

vector = tokenizer.encode('Очень удобно использовать уже готовый трансформатор текста', add_special_tokens=True)
n = 280
# англ. вектор с отступами
padded = np.array(vector + [0]*(n - len(vector)))
print(padded)

# Теперь поясним модели, что нули не несут значимой информации.
# Это нужно для компоненты модели, которая называется «внимание»
# (англ. attention). Отбросим эти токены и «создадим маску» для
# действительно важных токенов, то есть укажем нулевые и не нулевые
# значения:
attention_mask = np.where(padded != 0, 1, 0)
print(attention_mask.shape)
# (280,)

